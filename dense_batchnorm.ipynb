{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQd6yi6FbJBU",
    "outputId": "19446a5f-52a1-4f02-bce5-db14b83967d3",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install keras-tuner --upgrade\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "po0TUbgq2dq3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "import keras_tuner as kt\n",
    "from keras.layers import Layer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQec6G7l2dq5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fk4gx7Vk2dq6",
    "outputId": "a306b332-a5f1-406d-c35d-2abb8403e899",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "dataset = \"MNIST\"\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "if dataset == \"CIFAR10\":\n",
    "    input_shape = (32, 32, 3)\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Split the data\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train, y_train, test_size=0.20, shuffle=True\n",
    ")\n",
    "\n",
    "print(dataset)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_valid.shape[0], \"validation samples\")\n",
    "print(x_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTINFUYB2dq6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building the MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1e0kWG6ooWE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Building a custom layer for our random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint to fix/freeze all layer weights but some (two params per layer unit)\n",
    "# NB: Very slow, only did a constraint based solution because Keras does not support \"picking\" weigths to freeze atm.\n",
    "class FreezeWeights(tf.keras.constraints.Constraint):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        self.init_weights = tf.keras.initializers.HeNormal(seed=None)(\n",
    "            shape=self.shape\n",
    "        ).numpy()\n",
    "        self.trainable_weights_0 = []\n",
    "        self.trainable_weights_1 = []\n",
    "        rand_ints = random.sample(\n",
    "            range(0, self.shape[0] * self.shape[1]), 2 * self.shape[1]\n",
    "        )\n",
    "        # Map random ints to coordinates in x,y\n",
    "        for i in range(self.shape[0]):\n",
    "            for j in range(self.shape[1]):\n",
    "                if i * self.shape[1] + j in rand_ints:\n",
    "                    self.trainable_weights_0.append(i)\n",
    "                    self.trainable_weights_1.append(j)\n",
    "\n",
    "    def __call__(self, w):\n",
    "        new_w = np.copy(self.init_weights)\n",
    "        # For each unfrozen weigth, update\n",
    "        for i in range(len(self.trainable_weights_0)):\n",
    "            new_w[self.trainable_weights_0[i]][self.trainable_weights_1[i]] = w[\n",
    "                self.trainable_weights_0[i]\n",
    "            ][self.trainable_weights_1[i]].numpy()\n",
    "\n",
    "        return tf.convert_to_tensor(new_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When input and all hidden layers are the same width, this makes computation much faster than weight constraint.\n",
    "class CustomDenseRandomLayer(Layer):\n",
    "    def __init__(self, width=784):\n",
    "        super(CustomDenseRandomLayer, self).__init__()\n",
    "        self.Y = layers.Dense(\n",
    "            2, name=\"trainable_layer\", use_bias=False, kernel_initializer=\"he_normal\"\n",
    "        )\n",
    "        self.Z = layers.Dense(\n",
    "            self.width - 2,\n",
    "            name=\"non_trainable_layer\",\n",
    "            use_bias=False,\n",
    "            kernel_initializer=\"he_normal\",\n",
    "        )\n",
    "        self.Z.trainable = False\n",
    "\n",
    "    def call(self, x):\n",
    "        return layers.Concatenate()([self.Y(x), self.Z(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEtOacukJ6I_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Not used at this time. Kept for future work.\n",
    "class CustomRandomLayer(Layer):\n",
    "    def __init__(self, trainable=True):\n",
    "        self.trainable = trainable\n",
    "        super(CustomRandomLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Adding two weights with shape as number of channels to make two params per channel\n",
    "        self.w1 = self.add_weight(\n",
    "            name=\"w1\",\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=\"he_normal\",\n",
    "            trainable=self.trainable,\n",
    "        )\n",
    "        self.w2 = self.add_weight(\n",
    "            name=\"w1\",\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=\"he_normal\",\n",
    "            trainable=self.trainable,\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return (x * self.w1) + self.w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaR4ygGDJqHy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fc_model_builder(\n",
    "    units_dense_layer=10,\n",
    "    dense_layers=1,\n",
    "    dense=True,\n",
    "    bn=False,\n",
    "    random=False,\n",
    "    dense_out=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Wrapper for constructing models with different trainable layers and units in dense layers.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=input_shape))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Adding variable amount of hidden dense layers\n",
    "    for i in range(0, dense_layers):\n",
    "        if random:\n",
    "            # Concatingating layers in custom layer works when the hidden layers have same dim as input layer\n",
    "            if units_dense_layer == model.layers[-1].output_shape[1]:\n",
    "                model.add(\n",
    "                    CustomDenseRandomLayer(width=model.layers[-1].output_shape[1])\n",
    "                )\n",
    "            else:\n",
    "                # Create constraint to fix all weights except some\n",
    "                # NB: Very very slow! 10x full param computing time\n",
    "                model.add(\n",
    "                    layers.Dense(\n",
    "                        units_dense_layer,\n",
    "                        activation=None,\n",
    "                        kernel_initializer=\"he_normal\",\n",
    "                        use_bias=False,\n",
    "                        trainable=dense,\n",
    "                        kernel_constraint=FixWeights(\n",
    "                            shape=(model.layers[-1].output_shape[1], units_dense_layer)\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "            # We keep the normalizing layer, but without it having any params effecting the model\n",
    "            model.add(\n",
    "                layers.BatchNormalization(\n",
    "                    beta_initializer=\"zeros\",\n",
    "                    gamma_initializer=\"ones\",\n",
    "                    trainable=False,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            model.add(\n",
    "                layers.Dense(\n",
    "                    units_dense_layer,\n",
    "                    activation=None,\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                    use_bias=False,\n",
    "                    trainable=dense,\n",
    "                    kernel_constraint=FixWeights(\n",
    "                        shape=(model.layers[-1].output_shape[1], units_dense_layer)\n",
    "                    )\n",
    "                    if random\n",
    "                    else None,\n",
    "                )\n",
    "            )\n",
    "            if bn:\n",
    "                model.add(\n",
    "                    layers.BatchNormalization(\n",
    "                        beta_initializer=\"zeros\",\n",
    "                        gamma_initializer=RandomNormal(mean=0.0, stddev=1.0),\n",
    "                        trainable=True,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        model.add(layers.Activation(\"relu\"))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            num_classes,\n",
    "            activation=\"softmax\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            trainable=dense_out,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "685IamGL2dq7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fc_model_builder_dim_wrapper(\n",
    "    layer_width=784, min_hdl=2, max_hdl=4, num_hdl_interval=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper function for creating the models of interest with widht and depth as variables\n",
    "    \"\"\"\n",
    "    fc_models = {}\n",
    "\n",
    "    # Build nested dict with variable number of layered models\n",
    "    for i in range(min_hdl, max_hdl + 1, num_hdl_interval):\n",
    "        fc_models[i] = {}\n",
    "\n",
    "        model_fc_all_layers = fc_model_builder(\n",
    "            units_dense_layer=layer_width,\n",
    "            dense_layers=i,\n",
    "            dense=True,\n",
    "            bn=True,\n",
    "            random=False,\n",
    "            dense_out=True,\n",
    "        )\n",
    "        vanilla_fc_model = fc_model_builder(\n",
    "            units_dense_layer=layer_width,\n",
    "            dense_layers=i,\n",
    "            dense=True,\n",
    "            bn=False,\n",
    "            random=False,\n",
    "            dense_out=True,\n",
    "        )\n",
    "        model_fc_bn = fc_model_builder(\n",
    "            units_dense_layer=layer_width,\n",
    "            dense_layers=i,\n",
    "            dense=False,\n",
    "            bn=True,\n",
    "            random=False,\n",
    "            dense_out=False,\n",
    "        )\n",
    "        model_fc_bn_out = fc_model_builder(\n",
    "            units_dense_layer=layer_width,\n",
    "            dense_layers=i,\n",
    "            dense=False,\n",
    "            bn=True,\n",
    "            random=False,\n",
    "            dense_out=True,\n",
    "        )\n",
    "        model_fc_random = fc_model_builder(\n",
    "            units_dense_layer=layer_width,\n",
    "            dense_layers=i,\n",
    "            dense=True,\n",
    "            bn=False,\n",
    "            random=True,\n",
    "            dense_out=False,\n",
    "        )\n",
    "        model_fc_out = fc_model_builder(\n",
    "            units_dense_layer=layer_width,\n",
    "            dense_layers=i,\n",
    "            dense=False,\n",
    "            bn=False,\n",
    "            random=False,\n",
    "            dense_out=True,\n",
    "        )\n",
    "        model_fc_none = fc_model_builder(\n",
    "            units_dense_layer=layer_width,\n",
    "            dense_layers=i,\n",
    "            dense=False,\n",
    "            bn=False,\n",
    "            random=False,\n",
    "            dense_out=False,\n",
    "        )\n",
    "\n",
    "        # Save models in dict\n",
    "        fc_models[i] = {\n",
    "            \"model_fc_all_layers\": model_fc_all_layers,\n",
    "            # \"vanilla_fc_model\": vanilla_fc_model,\n",
    "            \"model_fc_bn\": model_fc_bn,\n",
    "            # \"model_fc_bn_out\": model_fc_bn_out,\n",
    "            \"model_fc_random\": model_fc_random,\n",
    "            \"model_fc_out\": model_fc_out,\n",
    "            \"model_fc_none\": model_fc_none,\n",
    "        }\n",
    "\n",
    "    return fc_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjUvgMqrJOpv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch in [70, 100]:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "\n",
    "# Creating callbacks\n",
    "log_dir = os.path.join(\"logs/\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_images=True,\n",
    "    )\n",
    "]\n",
    "callbacks.append(CSVLogger(log_dir + \"/\" + \"latest.csv\"))\n",
    "callbacks.append(\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
    "    )\n",
    ")\n",
    "callbacks.append(keras.callbacks.LearningRateScheduler(scheduler, verbose=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6Wh1Zimzl3W",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building models of different depth and trainable layers\n",
    "fc_models = fc_model_builder_dim_wrapper(\n",
    "    layer_width=784, min_hdl=2, max_hdl=14, num_hdl_interval=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "er1X8gFY9olm",
    "outputId": "c8211dfb-76d5-46dd-eab6-46604f48751b",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Init training params\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "model_histories = {}\n",
    "\n",
    "# Create new time-based log dir\n",
    "log_dir = os.path.join(\n",
    "    \"logs/dense\",\n",
    "    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    ")\n",
    "\n",
    "# Train all models in dict\n",
    "for depth, nested in fc_models.items():\n",
    "    model_histories[depth] = {}\n",
    "    log_dir_depth = log_dir + \"/\" + str(depth)\n",
    "\n",
    "    for key, model in nested.items():\n",
    "        # Reinitialize callbacks with new directory\n",
    "        log_dir_depth_key = log_dir_depth + \"/\" + key\n",
    "\n",
    "        callbacks[0] = keras.callbacks.TensorBoard(\n",
    "            log_dir=log_dir_depth_key,\n",
    "            histogram_freq=1,\n",
    "            write_graph=True,\n",
    "            write_images=True,\n",
    "        )\n",
    "        callbacks[1] = CSVLogger(log_dir + \"fit_csv_logger.csv\")\n",
    "\n",
    "        # Compile and fit models\n",
    "        model.compile(\n",
    "            loss=keras.losses.categorical_crossentropy,\n",
    "            optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
    "            metrics=[\"accuracy\"],\n",
    "            run_eagerly=True,\n",
    "        )\n",
    "        model_histories[depth][key] = model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_valid, y_valid),\n",
    "            callbacks=callbacks,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tL3nbOkDzuyG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir './logs/dense'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test dataset\n",
    "loss_test_dict = {}\n",
    "acc_test_dict = {}\n",
    "\n",
    "for depth, nested in fc_models.items():\n",
    "    acc_test_dict[depth] = {}\n",
    "    loss_test_dict[depth] = {}\n",
    "    for key, model in nested.items():\n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "        loss_test_dict[depth][key] = score[0]\n",
    "        acc_test_dict[depth][key] = score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn dict with acc metrics into dataframe and export\n",
    "acc_test_frame = pd.DataFrame.from_dict(acc_test_dict, orient=\"index\")\n",
    "loss_test_frame = pd.DataFrame.from_dict(loss_test_dict, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save statistics\n",
    "acc_test_frame.to_csv(\n",
    "    \"logs/dense_test_stats_\"\n",
    "    + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    + \".csv\",\n",
    "    index=True,\n",
    "    index_label=\"layers\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMp0AXBPz0KA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeHFdXCMzzRT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save all models\n",
    "for depth, nested in fc_models.items():\n",
    "    for key, model in nested.items():\n",
    "        model.save(\"models/saved764/\" + str(depth) + key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model('path/to/location')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3RZOmeyzUZh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXYObER0zzUt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model = keras.models.load_model('path/to/location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qT3QvFaBZKzQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MyFcHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = fc_model_builder(\n",
    "            dense=hp.Boolean(\"dense\"),\n",
    "            bn=hp.Boolean(\"bn\"),\n",
    "            random=not hp.Boolean(\"bn\"),\n",
    "            dense_out=hp.Boolean(\"output\"),\n",
    "        )\n",
    "        model.compile(\n",
    "            loss=keras.losses.categorical_crossentropy,\n",
    "            optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=128,  # hp.Choice(\"batch_size\", [16, 32, 64, 128])\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LjCNGGfZ3Kg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fc_tuner = kt.RandomSearch(\n",
    "    MyFcHyperModel(),\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,\n",
    "    overwrite=True,\n",
    "    # Set a directory to store the intermediate results.\n",
    "    directory=\"/tmp/tb\",\n",
    "    project_name=\"fc_batchnorm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSD4AJgAZ5Gs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fc_tuner.search(x_train, y_train, epochs=2, validation_data=(x_test, y_test), callbacks=[callbacks],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yKTDOlXiIYo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir ./logs/fc"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
